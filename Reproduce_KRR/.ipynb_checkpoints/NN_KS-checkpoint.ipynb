{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for the KS mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import optimizers\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from KRR_reproduce import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "EPOCHS = 50000          # maximum number of training epochs\n",
    "TEST_SIZE = 0.1     # train-test split\n",
    "OPTIMIZER = 'sgd'\n",
    "LEARNING_RATE = 0.001   # larger means faster learning, more aggressive, set this in [0.001 - 0.003]\n",
    "GRID_SPACE = 0.8        # 0.8 instead of 0.08 in the paper -- makes it faster and the input much smaller\n",
    "DECAY = 0.0             # decay learning rate as training advances\n",
    "SIM_NO = 150\n",
    "\n",
    "if GRID_SPACE == 0.08: \n",
    "    HIDDEN = (10,)\n",
    "    # NOT RUN YET, THIS IS VERY EXPENSIVE TO OPTIMIZE\n",
    "else: \n",
    "    HIDDEN = (35, 35)          # found through hyperparameter optimization\n",
    "    \n",
    "# path to data\n",
    "os.environ['PROJDIR'] = '/Users/simonbatzner1/Desktop/Research/Research_Code/ML-electron-density'\n",
    "STR_PREF = os.environ['PROJDIR'] + '/data/H2_DFT/temp_data/store/'\n",
    "\n",
    "# ignore tf warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "ens = []\n",
    "seps = []\n",
    "fours = []\n",
    "\n",
    "for n in range(SIM_NO):\n",
    "    # load separation, energy, and density\n",
    "    sep = np.load(STR_PREF + 'sep_store/sep' + str(n) + '.npy')\n",
    "    en = np.load(STR_PREF + 'en_store/en' + str(n) + '.npy')\n",
    "    four = np.load(STR_PREF + 'four_store/four' + str(n) + '.npy')\n",
    "\n",
    "    # put results in a nicer format\n",
    "    sep = np.reshape(sep, (1,))[0]\n",
    "    en = np.reshape(en, (1,))[0]['energy']\n",
    "    four = np.real(four)\n",
    "\n",
    "    # store quantities\n",
    "    ens.append(en)\n",
    "    seps.append(sep)\n",
    "    fours.append(four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_architecture(input_shape, hidden_size, summary, activation='relu'):\n",
    "    \"\"\"\n",
    "    Built Neural Network using Keras\n",
    "\n",
    "    :param input_shape: shape of the input data\n",
    "    :param hidden_size: tuple of number of hidden layers, eg. (30, 30, 40) builds a network with hidden layers 30-30-40\n",
    "    :param summary: boolean, true plots a summary\n",
    "    :param activation: activiation function\n",
    "    :return: keras Sequential model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    print(input_shape)\n",
    "\n",
    "    # hidden layers\n",
    "    model.add(Dense(hidden_size[0], input_shape=input_shape, activation=activation))\n",
    "    for layer_size in hidden_size[1:]:\n",
    "        model.add(Dense(layer_size, activation=activation))\n",
    "        model.add(Dropout(0.2))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, training_data, training_labels, validation_data, validation_labels, batchsize=64):\n",
    "    \"\"\"\"\n",
    "    Train Neural Network model\n",
    "    \"\"\"\n",
    "    history = model.fit(training_data, training_labels, validation_data=(validation_data, validation_labels),\n",
    "                        batch_size=batchsize,\n",
    "                        verbose=0, shuffle=True)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Gaussian Potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pots = []\n",
    "grid_len = 5.29177 * 2\n",
    "\n",
    "for n in range(SIM_NO):\n",
    "    dist = seps[n]\n",
    "    pot = pot_rep(dist, grid_len, grid_space=GRID_SPACE)\n",
    "    pot = pot.flatten()\n",
    "    pots.append(pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 2744)\n"
     ]
    }
   ],
   "source": [
    "data = pots\n",
    "labels = ens\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=TEST_SIZE, random_state=SEED)\n",
    "\n",
    "# keras input needs numpy ndarrays\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2744,)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 35)                96075     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 35)                1260      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 97,371\n",
      "Trainable params: 97,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build neural net\n",
    "model = init_architecture(input_shape=pots[0].shape, hidden_size=tuple(HIDDEN), summary=True,\n",
    "                          activation='tanh')\n",
    "\n",
    "# pick optimizer\n",
    "adam = optimizers.Adam(lr=LEARNING_RATE, decay=DECAY)\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "if OPTIMIZER == 'adam': \n",
    "    model.compile(optimizer=adam, loss='mean_squared_error', metrics=['mse'])\n",
    "elif OPTIMIZER == 'sgd': \n",
    "    model.compile(optimizer=sgd, loss='mean_squared_error', metrics=['mse'])\n",
    "else: \n",
    "    \"Default optimizer ADAM being used...\"\n",
    "    model.compile(optimizer=adam, loss='mean_squared_error', metrics=['mse'])\n",
    "    \n",
    "# Early stopping on validation error\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   min_delta=0,\n",
    "                   patience=500,\n",
    "                   verbose=10, mode='auto')\n",
    "\n",
    "# Log graph and gradients\n",
    "tb = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=1, write_graph=True, write_grads=True,\n",
    "                 write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "# Save training history\n",
    "history = model.fit(x_train, y_train, epochs=EPOCHS, verbose=0, validation_split=0.2, callbacks=[es, tb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "# Predict\n",
    "test_loss = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Eval on training data\n",
    "y_true, y_pred = y_train, model.predict(x_train)\n",
    "print(\"\\nMAE on training data: \\t{}\".format(mean_absolute_error(y_true, y_pred)))\n",
    "\n",
    "# Eval on test data\n",
    "y_true, y_pred = y_test, model.predict(x_test)\n",
    "print(\"MAe on test data: \\t{}\".format(mean_absolute_error(y_true, y_pred)))\n",
    "\n",
    "# Predict on new data\n",
    "print(\"\\n\\t\\tPred \\t| \\tTrue\\n\")\n",
    "print(np.c_[y_pred, y_true])\n",
    "\n",
    "# Results\n",
    "print(\"\\n\\nTest Loss: {}\".format(test_loss[1]))\n",
    "\n",
    "# Plot loss history\n",
    "print(history.history.keys())\n",
    "plt.semilogy(history.history['loss'])\n",
    "plt.semilogy(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [1] Brockherde et al. Bypassing the Kohn-Sham equations with machine learning. Nature Communications 8, 872 (2017)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
